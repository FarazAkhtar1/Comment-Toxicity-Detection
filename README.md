This is a model used to detect toxicity in batches/groups of comments. 
The model gives multiple outputs, which helps in detecting the kind of toxicity in the comments.
The six types that are detected by the model are: Toxic, Severe Toxic, Obscene, Threat, Insult, and Identity Hate.

You can find the dataset used in training the model here: https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data
