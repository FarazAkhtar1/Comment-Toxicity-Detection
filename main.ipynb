{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.16.1)\n",
      "Collecting tensorflow-gpu\n",
      "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.10.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: python_version>\"3.7\" in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (0.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n",
      "Building wheels for collected packages: tensorflow-gpu\n",
      "  Building wheel for tensorflow-gpu (setup.py): started\n",
      "  Building wheel for tensorflow-gpu (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for tensorflow-gpu\n",
      "Failed to build tensorflow-gpu\n",
      "Installing collected packages: tensorflow-gpu\n",
      "  Running setup.py install for tensorflow-gpu: started\n",
      "  Running setup.py install for tensorflow-gpu: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [18 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\user\\AppData\\Local\\Temp\\pip-install-afg3oud4\\tensorflow-gpu_1fe6bdc881b54f1387e7e018d90f3c4f\\setup.py\", line 37, in <module>\n",
      "          raise Exception(TF_REMOVAL_WARNING)\n",
      "      Exception:\n",
      "      \n",
      "      =========================================================\n",
      "      The \"tensorflow-gpu\" package has been removed!\n",
      "      \n",
      "      Please install \"tensorflow\" instead.\n",
      "      \n",
      "      Other than the name, the two packages have been identical\n",
      "      since TensorFlow 2.1, or roughly since Sep 2019. For more\n",
      "      information, see: pypi.org/project/tensorflow-gpu\n",
      "      =========================================================\n",
      "      \n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tensorflow-gpu\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Running setup.py install for tensorflow-gpu did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [18 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\user\\AppData\\Local\\Temp\\pip-install-afg3oud4\\tensorflow-gpu_1fe6bdc881b54f1387e7e018d90f3c4f\\setup.py\", line 37, in <module>\n",
      "          raise Exception(TF_REMOVAL_WARNING)\n",
      "      Exception:\n",
      "      \n",
      "      =========================================================\n",
      "      The \"tensorflow-gpu\" package has been removed!\n",
      "      \n",
      "      Please install \"tensorflow\" instead.\n",
      "      \n",
      "      Other than the name, the two packages have been identical\n",
      "      since TensorFlow 2.1, or roughly since Sep 2019. For more\n",
      "      information, see: pypi.org/project/tensorflow-gpu\n",
      "      =========================================================\n",
      "      \n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "× Encountered error while trying to install package.\n",
      "╰─> tensorflow-gpu\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow tensorflow-gpu pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\Python\\\\Comment Toxicity\\\\jigsaw-toxic-comment-classification-challenge\\\\train.csv\\\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Explanation\\nWhy the edits made under my usern...\n",
       "1         D'aww! He matches this background colour I'm s...\n",
       "2         Hey man, I'm really not trying to edit war. It...\n",
       "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4         You, sir, are my hero. Any chance you remember...\n",
       "                                ...                        \n",
       "159566    \":::::And for the second time of asking, when ...\n",
       "159567    You should be ashamed of yourself \\n\\nThat is ...\n",
       "159568    Spitzer \\n\\nUmm, theres no actual article for ...\n",
       "159569    And it looks like it was actually you who put ...\n",
       "159570    \"\\nAnd ... I really don't think you understand...\n",
       "Name: comment_text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[2:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['comment_text']\n",
    "y = df[df.columns[2:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 170000 #no of words that will be stored in dictionary as tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(max_tokens= MAX_WORDS, output_sequence_length=1800, output_mode= 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       "       \"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\",\n",
       "       \"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\",\n",
       "       ...,\n",
       "       'Best known social networking websites \\n\\nWhat are the best known social networking websites beside MySpace? 81.223.23.159',\n",
       "       \"Page name \\n\\nA user has reverted an edit that was going to include a UAV in the list.  The reason stated was that 'A UAV isn't a specific aircraft and the Army isn't part of the RAAF'.  There are quite a few aircraft in the list that are not part of the RAAF.  Moreover, all future helicopter purchases are most likely going to be either Army or RAN .  Therefore, we either remove all RAN and Army aircraft or we move the page to 'List of aircraft of the ADF'.  If you're up to it we can vote to move the page as below and I will do any changes (if any) next week (Mon 19th March).  -   \\nsupport - it will make the list more comprehensive and keep the current intent.\",\n",
       "       \"Shutup \\n\\nU ain't nobody, U ugly gay fag,\"], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'to',\n",
       " 'of',\n",
       " 'and',\n",
       " 'i',\n",
       " 'a',\n",
       " 'you',\n",
       " 'is',\n",
       " 'that',\n",
       " 'in',\n",
       " 'it',\n",
       " 'for',\n",
       " 'this',\n",
       " 'not',\n",
       " 'on',\n",
       " 'be',\n",
       " 'as',\n",
       " 'have',\n",
       " 'are',\n",
       " 'with',\n",
       " 'your',\n",
       " 'if',\n",
       " 'was',\n",
       " 'article',\n",
       " 'or',\n",
       " 'but',\n",
       " 'page',\n",
       " 'an',\n",
       " 'my',\n",
       " 'by',\n",
       " 'from',\n",
       " 'do',\n",
       " 'at',\n",
       " 'me',\n",
       " 'so',\n",
       " 'about',\n",
       " 'what',\n",
       " 'can',\n",
       " 'wikipedia',\n",
       " 'all',\n",
       " 'there',\n",
       " 'has',\n",
       " 'would',\n",
       " 'its',\n",
       " 'will',\n",
       " 'like',\n",
       " 'no',\n",
       " 'one',\n",
       " 'just',\n",
       " 'talk',\n",
       " 'please',\n",
       " 'they',\n",
       " 'any',\n",
       " 'he',\n",
       " 'we',\n",
       " 'been',\n",
       " 'which',\n",
       " 'dont',\n",
       " 'here',\n",
       " 'should',\n",
       " 'other',\n",
       " 'more',\n",
       " 'see',\n",
       " 'some',\n",
       " 'his',\n",
       " 'who',\n",
       " 'also',\n",
       " 'im',\n",
       " 'think',\n",
       " 'ass',\n",
       " 'how',\n",
       " 'because',\n",
       " 'know',\n",
       " 'why',\n",
       " 'out',\n",
       " 'edit',\n",
       " 'were',\n",
       " 'am',\n",
       " 'fuck',\n",
       " 'then',\n",
       " 'people',\n",
       " 'only',\n",
       " 'go',\n",
       " 'up',\n",
       " 'use',\n",
       " 'articles',\n",
       " 'now',\n",
       " 'hate',\n",
       " 'their',\n",
       " 'may',\n",
       " 'being',\n",
       " 'when',\n",
       " 'time',\n",
       " 'did',\n",
       " 'than',\n",
       " 'even',\n",
       " 'thanks',\n",
       " 'get',\n",
       " 'make',\n",
       " 'information',\n",
       " 'them',\n",
       " 'good',\n",
       " 'well',\n",
       " 'could',\n",
       " 'had',\n",
       " 'yourself',\n",
       " 'sources',\n",
       " 'want',\n",
       " 'very',\n",
       " 'first',\n",
       " 'deletion',\n",
       " 'way',\n",
       " 'made',\n",
       " 'does',\n",
       " 'name',\n",
       " 'new',\n",
       " 'such',\n",
       " 'say',\n",
       " 'really',\n",
       " 'section',\n",
       " 'pages',\n",
       " 'ive',\n",
       " 'again',\n",
       " 'need',\n",
       " 'where',\n",
       " 'used',\n",
       " 'these',\n",
       " 'user',\n",
       " 'much',\n",
       " 'edits',\n",
       " 'source',\n",
       " 'before',\n",
       " 'same',\n",
       " 'many',\n",
       " 'into',\n",
       " 'since',\n",
       " 'right',\n",
       " 'help',\n",
       " 'find',\n",
       " 'after',\n",
       " 'those',\n",
       " 'said',\n",
       " 'thank',\n",
       " 'u',\n",
       " 'read',\n",
       " 'most',\n",
       " 'someone',\n",
       " 'editing',\n",
       " 'still',\n",
       " 'two',\n",
       " 'discussion',\n",
       " 'too',\n",
       " 'image',\n",
       " 'own',\n",
       " 'fact',\n",
       " 'take',\n",
       " 'look',\n",
       " 'something',\n",
       " 'him',\n",
       " 'work',\n",
       " 'link',\n",
       " 'going',\n",
       " 'thats',\n",
       " 'deleted',\n",
       " 'point',\n",
       " 'editors',\n",
       " 'back',\n",
       " 'better',\n",
       " 'stop',\n",
       " 'content',\n",
       " 'under',\n",
       " 'over',\n",
       " 'never',\n",
       " 'case',\n",
       " 'might',\n",
       " 'youre',\n",
       " 'utc',\n",
       " 'personal',\n",
       " 'history',\n",
       " 'sure',\n",
       " 'block',\n",
       " 'another',\n",
       " 'list',\n",
       " 'bot',\n",
       " 'us',\n",
       " 'removed',\n",
       " 'our',\n",
       " 'without',\n",
       " 'place',\n",
       " 'note',\n",
       " 'added',\n",
       " 'put',\n",
       " 'done',\n",
       " 'doesnt',\n",
       " 'cant',\n",
       " 'comment',\n",
       " 'blocked',\n",
       " 'both',\n",
       " 'add',\n",
       " 'person',\n",
       " 'didnt',\n",
       " 'using',\n",
       " 'however',\n",
       " 'thing',\n",
       " 'anything',\n",
       " 'useless',\n",
       " 'question',\n",
       " 'ask',\n",
       " 'come',\n",
       " 'against',\n",
       " 'reason',\n",
       " 'feel',\n",
       " 'while',\n",
       " 'vandalism',\n",
       " 'part',\n",
       " 'though',\n",
       " 'welcome',\n",
       " 'actually',\n",
       " 'things',\n",
       " 'policy',\n",
       " 'ill',\n",
       " 'hope',\n",
       " 'problem',\n",
       " 'off',\n",
       " 'her',\n",
       " 'believe',\n",
       " 'post',\n",
       " 'little',\n",
       " 'links',\n",
       " 'sorry',\n",
       " 'anyone',\n",
       " 'seems',\n",
       " 'best',\n",
       " 'must',\n",
       " 'free',\n",
       " 'making',\n",
       " 'change',\n",
       " 'got',\n",
       " 'years',\n",
       " 'rather',\n",
       " 'far',\n",
       " 'users',\n",
       " 'remove',\n",
       " 'isnt',\n",
       " 'mean',\n",
       " 'different',\n",
       " 'wiki',\n",
       " 'others',\n",
       " 'reference',\n",
       " 'editor',\n",
       " 'understand',\n",
       " 'trying',\n",
       " 'nothing',\n",
       " 'keep',\n",
       " 'agree',\n",
       " 'reliable',\n",
       " 'subject',\n",
       " 'last',\n",
       " 'etc',\n",
       " 'above',\n",
       " 'hi',\n",
       " 'wrong',\n",
       " 'issue',\n",
       " 'world',\n",
       " 'long',\n",
       " 'fair',\n",
       " 'delete',\n",
       " 'already',\n",
       " 'references',\n",
       " 'original',\n",
       " 'great',\n",
       " 'give',\n",
       " 'few',\n",
       " 'example',\n",
       " 'write',\n",
       " 'through',\n",
       " 'simply',\n",
       " 'says',\n",
       " 'every',\n",
       " 'she',\n",
       " 'either',\n",
       " 'comments',\n",
       " 'bit',\n",
       " 'try',\n",
       " 'let',\n",
       " 'english',\n",
       " 'life',\n",
       " 'word',\n",
       " 'between',\n",
       " 'text',\n",
       " 'speedy',\n",
       " 'clearly',\n",
       " 'tag',\n",
       " 'evidence',\n",
       " 'yes',\n",
       " 'probably',\n",
       " 'opinion',\n",
       " 'leave',\n",
       " 'day',\n",
       " 'states',\n",
       " 'saying',\n",
       " 'having',\n",
       " 'enough',\n",
       " 'around',\n",
       " 'doing',\n",
       " 'hello',\n",
       " 'id',\n",
       " 'least',\n",
       " 'encyclopedia',\n",
       " 'yet',\n",
       " 'perhaps',\n",
       " 'called',\n",
       " 'site',\n",
       " 'questions',\n",
       " 'else',\n",
       " 'thought',\n",
       " 'number',\n",
       " 'further',\n",
       " 'found',\n",
       " 'support',\n",
       " 'important',\n",
       " 'down',\n",
       " 'real',\n",
       " 'message',\n",
       " 'clear',\n",
       " 'account',\n",
       " '2',\n",
       " 'once',\n",
       " 'ip',\n",
       " 'war',\n",
       " 'ever',\n",
       " 'book',\n",
       " '1',\n",
       " 'statement',\n",
       " 'show',\n",
       " 'state',\n",
       " 'request',\n",
       " 'lot',\n",
       " 'version',\n",
       " 'continue',\n",
       " 'consider',\n",
       " 'created',\n",
       " 'call',\n",
       " 'term',\n",
       " 'reverted',\n",
       " 'idea',\n",
       " 'check',\n",
       " 'notice',\n",
       " 'needs',\n",
       " 'images',\n",
       " 'several',\n",
       " 'maybe',\n",
       " 'whether',\n",
       " 'true',\n",
       " 'second',\n",
       " 'research',\n",
       " 'copyright',\n",
       " 'adding',\n",
       " 'material',\n",
       " 'claim',\n",
       " 'facts',\n",
       " 'given',\n",
       " 'changes',\n",
       " 'times',\n",
       " 'makes',\n",
       " 'matter',\n",
       " 'consensus',\n",
       " 'year',\n",
       " 'written',\n",
       " 'view',\n",
       " 'title',\n",
       " 'instead',\n",
       " 'correct',\n",
       " 'quite',\n",
       " 'mention',\n",
       " 'admin',\n",
       " 'address',\n",
       " 'tell',\n",
       " 'left',\n",
       " 'each',\n",
       " 'theres',\n",
       " 'criteria',\n",
       " 'words',\n",
       " 'current',\n",
       " 'contributions',\n",
       " 'bad',\n",
       " 'until',\n",
       " 'review',\n",
       " 'pov',\n",
       " 'following',\n",
       " 'considered',\n",
       " 'although',\n",
       " 'website',\n",
       " 'course',\n",
       " 'based',\n",
       " 'notable',\n",
       " '3',\n",
       " '—',\n",
       " 'wikipedias',\n",
       " 'old',\n",
       " 'main',\n",
       " 'looking',\n",
       " 'fucking',\n",
       " 'seem',\n",
       " 'possible',\n",
       " 'cannot',\n",
       " 'guidelines',\n",
       " 'explain',\n",
       " 'always',\n",
       " 'less',\n",
       " 'getting',\n",
       " 'claims',\n",
       " 'means',\n",
       " 'include',\n",
       " 'revert',\n",
       " 'move',\n",
       " 'man',\n",
       " 'language',\n",
       " 'changed',\n",
       " 'seen',\n",
       " 'issues',\n",
       " 'top',\n",
       " 'regarding',\n",
       " 'including',\n",
       " 'provide',\n",
       " 'project',\n",
       " 'mentioned',\n",
       " 'lead',\n",
       " 'later',\n",
       " 'care',\n",
       " 'three',\n",
       " 'rules',\n",
       " 'ok',\n",
       " 'myself',\n",
       " 'group',\n",
       " 'attack',\n",
       " 'political',\n",
       " 'currently',\n",
       " 'big',\n",
       " 'wasnt',\n",
       " 'template',\n",
       " 'stuff',\n",
       " 'specific',\n",
       " 'start',\n",
       " 'recent',\n",
       " 'sense',\n",
       " 'per',\n",
       " 'listed',\n",
       " 'date',\n",
       " 'whole',\n",
       " 'position',\n",
       " 'king',\n",
       " 'general',\n",
       " 'end',\n",
       " 'party',\n",
       " 'names',\n",
       " 'known',\n",
       " 'anyway',\n",
       " 'proposed',\n",
       " 'kind',\n",
       " 'interest',\n",
       " 'happy',\n",
       " 'according',\n",
       " 'redirect',\n",
       " 'attacks',\n",
       " 'able',\n",
       " 'wont',\n",
       " 'warning',\n",
       " 'taken',\n",
       " 'sentence',\n",
       " 'mind',\n",
       " 'included',\n",
       " 'create',\n",
       " 'suggest',\n",
       " 'picture',\n",
       " 'nor',\n",
       " 'relevant',\n",
       " 'oh',\n",
       " 'media',\n",
       " 'looks',\n",
       " 'full',\n",
       " 'country',\n",
       " 'hey',\n",
       " 'wish',\n",
       " 'school',\n",
       " 'public',\n",
       " 'order',\n",
       " 'answer',\n",
       " 'response',\n",
       " 'present',\n",
       " 'fucker',\n",
       " 'appears',\n",
       " 'within',\n",
       " 'summary',\n",
       " 'nice',\n",
       " 'interested',\n",
       " 'completely',\n",
       " 'common',\n",
       " 'wanted',\n",
       " 'obviously',\n",
       " 'news',\n",
       " 'homo',\n",
       " '2005',\n",
       " 'writing',\n",
       " 'username',\n",
       " 'single',\n",
       " 'pretty',\n",
       " 'line',\n",
       " 'lets',\n",
       " 'email',\n",
       " '4',\n",
       " 'youve',\n",
       " 'working',\n",
       " 'romney',\n",
       " 'related',\n",
       " 'quote',\n",
       " 'mitt',\n",
       " 'form',\n",
       " 'during',\n",
       " 'become',\n",
       " 'days',\n",
       " 'appropriate',\n",
       " 'truth',\n",
       " 'info',\n",
       " 'government',\n",
       " 'talking',\n",
       " 'style',\n",
       " 'everything',\n",
       " 'topic',\n",
       " 'policies',\n",
       " 'past',\n",
       " 'company',\n",
       " 'ago',\n",
       " 'today',\n",
       " 'sort',\n",
       " 'side',\n",
       " 'removing',\n",
       " 'remember',\n",
       " 'noticed',\n",
       " 'next',\n",
       " 'neutral',\n",
       " 'future',\n",
       " 'certainly',\n",
       " 'away',\n",
       " 'united',\n",
       " 'started',\n",
       " 'sign',\n",
       " 'official',\n",
       " 'knowledge',\n",
       " 'havent',\n",
       " '5',\n",
       " 'major',\n",
       " 'love',\n",
       " 'game',\n",
       " 'discuss',\n",
       " 'cocksucker',\n",
       " 'wp',\n",
       " 'unless',\n",
       " 'reasons',\n",
       " 'itself',\n",
       " 'came',\n",
       " 'tried',\n",
       " 'therefore',\n",
       " 'taking',\n",
       " 'sucks',\n",
       " 'similar',\n",
       " 'published',\n",
       " 'process',\n",
       " 'placed',\n",
       " 'mothjer',\n",
       " 'guess',\n",
       " 'community',\n",
       " 'american',\n",
       " '2007',\n",
       " '•',\n",
       " 'wrote',\n",
       " 'stupid',\n",
       " 'stated',\n",
       " 'ie',\n",
       " 'four',\n",
       " 'false',\n",
       " 'everyone',\n",
       " 'conflict',\n",
       " 'cities',\n",
       " 'cheers',\n",
       " '2008',\n",
       " 'soon',\n",
       " 'short',\n",
       " 'reverting',\n",
       " 'reading',\n",
       " 'notability',\n",
       " 'guy',\n",
       " 'film',\n",
       " 'faith',\n",
       " 'especially',\n",
       " 'banned',\n",
       " 'argument',\n",
       " 'story',\n",
       " 'open',\n",
       " 'web',\n",
       " 'views',\n",
       " 'system',\n",
       " 'report',\n",
       " 'reply',\n",
       " 'paragraph',\n",
       " 'improve',\n",
       " 'dispute',\n",
       " 'definition',\n",
       " 'asked',\n",
       " 'appreciate',\n",
       " '2006',\n",
       " 'works',\n",
       " 'whatever',\n",
       " 'posted',\n",
       " 'music',\n",
       " 'exactly',\n",
       " 'along',\n",
       " 'shit',\n",
       " 'hard',\n",
       " 'entry',\n",
       " 'due',\n",
       " 'below',\n",
       " 'almost',\n",
       " 'actual',\n",
       " 'yeah',\n",
       " 'obvious',\n",
       " 'lion',\n",
       " 'likely',\n",
       " 'gay',\n",
       " 'deleting',\n",
       " 'cited',\n",
       " 'sourced',\n",
       " 'north',\n",
       " 'moved',\n",
       " 'involved',\n",
       " 'internet',\n",
       " 'explanation',\n",
       " 'certain',\n",
       " 'stay',\n",
       " 'rule',\n",
       " 'books',\n",
       " 'bestfrozen',\n",
       " 'search',\n",
       " 'provided',\n",
       " 'often',\n",
       " 'learn',\n",
       " 'hes',\n",
       " 'entire',\n",
       " 'edited',\n",
       " 'cause',\n",
       " 'attempt',\n",
       " 'university',\n",
       " 'problems',\n",
       " 'npov',\n",
       " 'india',\n",
       " 'citations',\n",
       " 'band',\n",
       " 'admins',\n",
       " 'white',\n",
       " 'theory',\n",
       " 'themselves',\n",
       " 'team',\n",
       " 'shows',\n",
       " 'regards',\n",
       " 'mr',\n",
       " 'live',\n",
       " 'law',\n",
       " 'himself',\n",
       " 'cite',\n",
       " 'avoid',\n",
       " 'went',\n",
       " 'various',\n",
       " 'vandalize',\n",
       " 'took',\n",
       " 'statements',\n",
       " 'small',\n",
       " 'members',\n",
       " 'external',\n",
       " '2004',\n",
       " 'sandbox',\n",
       " 'play',\n",
       " 'otherwise',\n",
       " 'months',\n",
       " 'interesting',\n",
       " 'high',\n",
       " 'goes',\n",
       " 'generally',\n",
       " 'description',\n",
       " 'city',\n",
       " 'british',\n",
       " 'area',\n",
       " 'alone',\n",
       " 'allowed',\n",
       " 'violation',\n",
       " 'valid',\n",
       " 'third',\n",
       " 'status',\n",
       " 'period',\n",
       " 'ones',\n",
       " 'multiple',\n",
       " 'indeed',\n",
       " 'exist',\n",
       " 'addition',\n",
       " 'thus',\n",
       " 'south',\n",
       " 'science',\n",
       " 'proper',\n",
       " 'particular',\n",
       " 'modern',\n",
       " 'factual',\n",
       " 'death',\n",
       " 'culture',\n",
       " 'comes',\n",
       " 'aware',\n",
       " 'assume',\n",
       " 'type',\n",
       " 'study',\n",
       " 'run',\n",
       " 'historical',\n",
       " 'follow',\n",
       " 'citation',\n",
       " 'calling',\n",
       " 'appear',\n",
       " 'apparently',\n",
       " 'accurate',\n",
       " 'rights',\n",
       " 'republic',\n",
       " 'propaganda',\n",
       " 'hand',\n",
       " 'french',\n",
       " 'disagree',\n",
       " 'children',\n",
       " 'available',\n",
       " 'allow',\n",
       " '–',\n",
       " 'towns',\n",
       " 'saw',\n",
       " 'meant',\n",
       " 'legal',\n",
       " 'guys',\n",
       " 'google',\n",
       " 'context',\n",
       " 'author',\n",
       " '6',\n",
       " 'vote',\n",
       " 'villages',\n",
       " 'useful',\n",
       " 'series',\n",
       " 'result',\n",
       " 'needed',\n",
       " 'job',\n",
       " 'helpful',\n",
       " 'category',\n",
       " 'actions',\n",
       " 'worked',\n",
       " 'seriously',\n",
       " 'previous',\n",
       " 'power',\n",
       " 'majority',\n",
       " 'hell',\n",
       " 'greek',\n",
       " 'automatically',\n",
       " 'attention',\n",
       " 'afd',\n",
       " 'accept',\n",
       " '2010',\n",
       " 'wouldnt',\n",
       " 'w',\n",
       " 'theyre',\n",
       " 'set',\n",
       " 'serious',\n",
       " 'nonsense',\n",
       " 'national',\n",
       " 'hours',\n",
       " 'simple',\n",
       " 'refer',\n",
       " 'messages',\n",
       " 'living',\n",
       " 'incorrect',\n",
       " 'george',\n",
       " 'five',\n",
       " 'events',\n",
       " 'despite',\n",
       " 'contributing',\n",
       " 'biased',\n",
       " 'arguments',\n",
       " 'album',\n",
       " 'administrator',\n",
       " 'upon',\n",
       " 're',\n",
       " 'points',\n",
       " 'head',\n",
       " 'god',\n",
       " 'fine',\n",
       " 'direct',\n",
       " 'contribs',\n",
       " 'changing',\n",
       " 'bush',\n",
       " 'youll',\n",
       " 'week',\n",
       " 'uploaded',\n",
       " 'standard',\n",
       " 'specifically',\n",
       " 'speak',\n",
       " 'quality',\n",
       " 'personally',\n",
       " 'online',\n",
       " 'face',\n",
       " 'earlier',\n",
       " 'debate',\n",
       " 'dead',\n",
       " 'de',\n",
       " 'august',\n",
       " 'whats',\n",
       " 'understanding',\n",
       " 'unblock',\n",
       " 'test',\n",
       " 'home',\n",
       " 'happened',\n",
       " 'family',\n",
       " 'entirely',\n",
       " 'contact',\n",
       " 'college',\n",
       " 'business',\n",
       " 'arent',\n",
       " 'america',\n",
       " 'academic',\n",
       " 'readers',\n",
       " 'press',\n",
       " 'minor',\n",
       " 'longer',\n",
       " 'july',\n",
       " 'doubt',\n",
       " 'blocking',\n",
       " 'administrators',\n",
       " 'accepted',\n",
       " '10',\n",
       " 'terms',\n",
       " 'supposed',\n",
       " 'sites',\n",
       " 'separate',\n",
       " 'rationale',\n",
       " 'proof',\n",
       " 'lack',\n",
       " 'gets',\n",
       " 'explaining',\n",
       " 'except',\n",
       " 'enjoy',\n",
       " 'details',\n",
       " 'creating',\n",
       " 'accounts',\n",
       " 'uses',\n",
       " 'told',\n",
       " 'together',\n",
       " 'thinking',\n",
       " 'sound',\n",
       " 'none',\n",
       " 'manual',\n",
       " 'lol',\n",
       " 'local',\n",
       " 'german',\n",
       " 'decided',\n",
       " 'copy',\n",
       " 'controversial',\n",
       " 'bias',\n",
       " '7',\n",
       " 'wikiproject',\n",
       " 'rest',\n",
       " 'respect',\n",
       " 'primary',\n",
       " 'meaning',\n",
       " 'lists',\n",
       " 'indicate',\n",
       " 'heard',\n",
       " 'existing',\n",
       " 'december',\n",
       " 'complete',\n",
       " 'age',\n",
       " 'totally',\n",
       " 'tagged',\n",
       " 'sometimes',\n",
       " 'sock',\n",
       " 'release',\n",
       " 'record',\n",
       " 'photo',\n",
       " 'nazi',\n",
       " 'lost',\n",
       " 'light',\n",
       " 'honest',\n",
       " 'games',\n",
       " 'faggot',\n",
       " 'deal',\n",
       " 'black',\n",
       " 'barnstar',\n",
       " 'asking',\n",
       " '8',\n",
       " 'standards',\n",
       " 'possibly',\n",
       " 'particularly',\n",
       " 'march',\n",
       " 'looked',\n",
       " 'jews',\n",
       " 'independent',\n",
       " 'ideas',\n",
       " 'fun',\n",
       " 'figure',\n",
       " 'easily',\n",
       " 'church',\n",
       " 'christian',\n",
       " 'c',\n",
       " 'box',\n",
       " 'tags',\n",
       " 'space',\n",
       " 'posting',\n",
       " 'month',\n",
       " 'june',\n",
       " 'human',\n",
       " 'experience',\n",
       " 'europe',\n",
       " 'eg',\n",
       " 'early',\n",
       " 'directly',\n",
       " 'decide',\n",
       " 'countries',\n",
       " 'contribute',\n",
       " 'contest',\n",
       " 'california',\n",
       " '20',\n",
       " '15',\n",
       " 'wikipedian',\n",
       " 'video',\n",
       " 'usernhrhs2010',\n",
       " 'un',\n",
       " 'towards',\n",
       " 'shall',\n",
       " 'service',\n",
       " 'sections',\n",
       " 'giving',\n",
       " 'funny',\n",
       " 'fix',\n",
       " 'criticism',\n",
       " 'couple',\n",
       " 'concerned',\n",
       " 'coming',\n",
       " 'battle',\n",
       " 'ban',\n",
       " 'act',\n",
       " 'across',\n",
       " '2009',\n",
       " 'userenigmaman',\n",
       " 'students',\n",
       " 'social',\n",
       " 'situation',\n",
       " 'shouldnt',\n",
       " 'outside',\n",
       " 'necessary',\n",
       " 'merely',\n",
       " 'level',\n",
       " 'ignore',\n",
       " 'heres',\n",
       " 'friends',\n",
       " 'described',\n",
       " 'dear',\n",
       " 'century',\n",
       " 'bring',\n",
       " 'born',\n",
       " 'websites',\n",
       " 'verifiable',\n",
       " 'takes',\n",
       " 'size',\n",
       " 'refers',\n",
       " 'recently',\n",
       " 'racist',\n",
       " 'posts',\n",
       " 'places',\n",
       " 'nobody',\n",
       " 'license',\n",
       " 'john',\n",
       " 'gave',\n",
       " 'friend',\n",
       " 'final',\n",
       " 'featured',\n",
       " 'exists',\n",
       " 'east',\n",
       " 'difference',\n",
       " 'close',\n",
       " 'access',\n",
       " 'wants',\n",
       " 'usually',\n",
       " 'tildes',\n",
       " 'region',\n",
       " 'realize',\n",
       " 'random',\n",
       " 'putting',\n",
       " 'piece',\n",
       " 'linked',\n",
       " 'league',\n",
       " 'inclusion',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1800,), dtype=int64, numpy=array([ 30, 116,   9, ...,   0,   0,   0], dtype=int64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(\"my name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1800,), dtype=int64, numpy=array([38,  9, 30, ...,  0,  0,  0], dtype=int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(\"what is my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text = vectorizer(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6000, 1800), dtype=int64, numpy=\n",
       "array([[  638,    75,     2, ...,     0,     0,     0],\n",
       "       [26444,    55,  1842, ...,     0,     0,     0],\n",
       "       [  484,   418,    69, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  234,   458,   953, ...,     0,     0,     0],\n",
       "       [   28,   116,     7, ...,     0,     0,     0],\n",
       "       [17302,   145,  2151, ...,     0,     0,     0]], dtype=int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text, y))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(160000)\n",
    "dataset = dataset.batch(16)\n",
    "dataset = dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1800)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset) #this number is in batches of 16, meaning the acual lenght is 9974*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.take(int(len(dataset)*.7))\n",
    "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2))\n",
    "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n",
      "75\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 4132, 24519,     9, ...,     0,     0,     0],\n",
       "        [24774,   283,   791, ...,     0,     0,     0],\n",
       "        [    2,  1054,    10, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [   14,    28,    61, ...,     0,     0,     0],\n",
       "        [  465, 16226,  1983, ...,     0,     0,     0],\n",
       "        [   75,     9,    12, ...,     0,     0,     0]], dtype=int64),\n",
       " array([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]], dtype=int64))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.as_numpy_iterator().next()  #next() cuz of 1 batch of 16 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Sequential Model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#Embedding layer, where integer indices representing tokens are mapped to vectors of fixed sized, aka word embeddings\n",
    "model.add(Embedding(MAX_WORDS+1,32 ))\n",
    "#Bidirectional layer because words that came before the current word can change the meaning of the given sentence, tanh cuz lstm\n",
    "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
    "#Features are extracted here\n",
    "model.add(Dense(128, activation= 'relu'))\n",
    "model.add(Dense(256, activation= 'relu'))\n",
    "model.add(Dense(128, activation= 'relu'))\n",
    "#Final layer\n",
    "model.add(Dense(6, activation= 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 991ms/step - accuracy: 0.6933 - loss: 0.2294 - val_accuracy: 0.9967 - val_loss: 0.0816\n",
      "Epoch 2/5\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 957ms/step - accuracy: 0.9913 - loss: 0.0787 - val_accuracy: 0.9950 - val_loss: 0.0560\n",
      "Epoch 3/5\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 974ms/step - accuracy: 0.9927 - loss: 0.0581 - val_accuracy: 0.9942 - val_loss: 0.0455\n",
      "Epoch 4/5\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 1s/step - accuracy: 0.9109 - loss: 0.0485 - val_accuracy: 0.9917 - val_loss: 0.0370\n",
      "Epoch 5/5\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 983ms/step - accuracy: 0.8848 - loss: 0.0396 - val_accuracy: 0.9925 - val_loss: 0.0381\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train, epochs=5, validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.metrics import Precision, Recall, CategoricalAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n"
     ]
    }
   ],
   "source": [
    "for batch in test.as_numpy_iterator(): \n",
    "    # Unpack the batch \n",
    "    X_true, y_true = batch\n",
    "    # Make a prediction \n",
    "    yhat = model.predict(X_true)\n",
    "    \n",
    "    # Flatten the predictions\n",
    "    y_true = y_true.flatten()\n",
    "    yhat = yhat.flatten()\n",
    "    \n",
    "    pre.update_state(y_true, yhat)\n",
    "    re.update_state(y_true, yhat)\n",
    "    acc.update_state(y_true, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8440366983413696, Recall:0.8288288116455078, Accuracy:0.6216216087341309\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision: {pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
